{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88926548-eacb-43b3-8f39-b65cc0d0c632",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0dc66-f0ba-4761-8e85-5f45b81bb1d4",
   "metadata": {},
   "source": [
    "### Counting approach for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bcccce3-eec0-4f92-b9e1-94754b9064c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chars\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos\n",
    "\n",
    "import torch\n",
    "N = torch.zeros((27,27,27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cfe389b-8c2e-4b29-bba2-86df32ac6bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74600f32-98f3-47ae-b873-494764971c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d7b78aa-f8e1-402e-9aec-882dbef7d1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce.\n",
      "bra.\n",
      "jalius.\n",
      "rochityharlonimittain.\n",
      "luwak.\n",
      "ka.\n",
      "da.\n",
      "samiyah.\n",
      "javer.\n",
      "gotai.\n",
      "moriellavojkwuthda.\n",
      "kaley.\n",
      "maside.\n",
      "en.\n",
      "aviyah.\n",
      "fobspihiliven.\n",
      "tahlasuzusfxx.\n",
      "an.\n",
      "glhpynn.\n",
      "isan.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    iy = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        p = P[ix, iy, :]\n",
    "        #p = N[ix].float()\n",
    "        #p = p / p.sum()\n",
    "        \n",
    "        iz = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[iz])\n",
    "        ix = iy\n",
    "        iy = iz\n",
    "        if iz == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ccc4042-8d80-4257-9374-0a8034e8e849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll=tensor(504653.)\n",
      "nll/n=tensor(2.2120)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        prob = P[ix1, ix2, ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1606f-a141-48ea-a080-67d129320a1a",
   "metadata": {},
   "source": [
    "### Neural net approach for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37a3a535-7b7a-48f1-8452-5738343dfea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars\n",
    "\n",
    "# want an encoding for bigrams\n",
    "# input to NN has length 27*27\n",
    "b = [ch1+ch2 for ch1 in ['.']+chars for ch2 in ['.']+chars]\n",
    "btoi = {b:i for i,b in enumerate(b)} # bigram to integer\n",
    "\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32836348-b9c0-4e59-8181-74a3ce496d25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        b = ch1 + ch2\n",
    "        ib = btoi[b]\n",
    "        iy = stoi[ch3]\n",
    "        xs.append(ib)\n",
    "        ys.append(iy)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator = g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d5a38d6-ba5d-4d85-b2ea-114721071d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "88122cb2-9e4e-422d-8dd5-379526c6ea7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (228146x729 and 27x27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     xenc \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(xs, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m729\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m xenc \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m      5\u001b[0m     counts \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m      6\u001b[0m     probs \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (228146x729 and 27x27)"
     ]
    }
   ],
   "source": [
    "for k in range(50):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=729).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims = True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() #  + 0.0001*(W**2).mean()\n",
    "    \n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += - 100* W.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86893727-8ff1-4849-b194-0bb7a6ca2a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sampling\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = ['.', '.']\n",
    "    ix = btoi['..']\n",
    "    while True:\n",
    "        ix = ''.join(out[-2:])\n",
    "        ix = btoi[''.join(out[-2:])]\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=729).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement = True, generator = g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "        \n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbaf08e-effb-44ae-b0ae-12518df560be",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc98d1-2e82-4aa7-af4c-bf3cedadf2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232fbce5-fec8-4f03-b147-b7158a9b4dbc",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. \n",
    "try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36129289-0045-4390-95f3-668b891fe370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524bc637-f242-49f1-a224-80d7eeb4fea8",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab041d1-f33e-432f-abfe-c7d365755456",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = btoi['nl'] # test bigram\n",
    "logits1 = W[ix, :]\n",
    "\n",
    "xenc = F.one_hot(torch.tensor([ix]), num_classes=729).float()\n",
    "logits2 = xenc @ W\n",
    "\n",
    "logits1 == logits2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819284aa-e821-46e1-8654-c3237de3754a",
   "metadata": {},
   "source": [
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d015755-666b-4085-be11-f3dfd55b7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe different gradients make a difference for the backpropagation/weights updating step\n",
    "# run pytoch examples\n",
    "# try to figure out where log's are missing to get the same results\n",
    "# write down pytorch formulas for special case\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae800b-fe0d-41b2-af7f-3c1ce8cab383",
   "metadata": {},
   "source": [
    "### Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf655c1-1c62-469b-8e80-4f2e9042d858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross entropy\n",
    "# Example where target contains class indices\n",
    "# This is case 1 in\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "# \"Note that this case is equivalent to the combination of LogSoftmax and NLLLoss.\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)        # 3 rows 5 cols sampled from standard normal distribution\n",
    "target = torch.empty(3, dtype=torch.long).random_(5) # length 3 vector of random integers uniform in [0, 5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b8357-27ef-4409-9725-d7c76bfdf4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input)\n",
    "print(target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a84d59-0c05-4ead-876d-0ccbed5df9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# softmax on rows for class probabilities\n",
    "classprobs = input.exp() / input.exp().sum(1, keepdims=True) # or just input.softmax(dim=1)\n",
    "classprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf06fc6-731e-4419-904d-6ce68213f54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "-classprobs[torch.arange(3), target].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae56dfc-87d8-418b-87c8-5f78f893c74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input2 = torch.randn(3, 5, requires_grad=True)\n",
    "target2 = torch.randn(3, 5).softmax(dim=1)\n",
    "output2 = loss(input2, target2)\n",
    "output2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229874e-001e-4d3a-86f2-758492fa3327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input2)\n",
    "print(target2)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ea488-e938-4410-9a4c-75e114516b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "-(input2.softmax(dim=1).log() * target2).sum(dim=1).mean()\n",
    "# this takes into account all probabilities, \n",
    "# not only the probabilities of the most likely class (uses more information than the first case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c8700-fa22-4f93-8a5e-b2b0fe5bb3ed",
   "metadata": {},
   "source": [
    "### Negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53e453-3062-480a-9801-e016241ec743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1) # m(input3) is numerically the same as input3.softmax(dim=1).log()\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input3 = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target3 = torch.tensor([1, 0, 4])\n",
    "output3 = loss(m(input3), target3)\n",
    "output3.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359a80a-8980-4a14-9274-2cefabff1f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input3)\n",
    "print(m(input3))\n",
    "print(target3)\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca498c2-2a96-4409-9277-378448e39544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LogSoftmax is literally softmax().log() (at least for this example)\n",
    "input3.softmax(dim=1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a31c1-da26-412c-bf20-04c3f7877e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "-m(input3)[torch.arange(3), target3].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "693107bc-cba3-4fcb-b404-e975655767e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "# input is standard normally distributed \n",
    "# of size N x C x height x width (N examples, C colors or classes (?), image height and image width)\n",
    "data = torch.randn(N, 16, 10, 10) # it says C but C is 4, and here input is 16\n",
    "conv = nn.Conv2d(16, C, (3, 3)) # something like: 16 colors in, C = 4 classes out, connect squares of size 3 by 3 pixels\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= integer classes value < C\n",
    "# output from conv has dimension 10 - (3-1) = 8, the number of different (but overlapping) 'windows' of length 3\n",
    "target4 = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output4 = loss(m(conv(data)), target4) # how does loss know the appropriate dimensions to for each operation?\n",
    "output4.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9d50a99b-c837-42a0-844e-560039f1e7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a79202-aff8-4f57-b4d5-cd8856f19d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nll = 0\n",
    "for n in range(5):\n",
    "    for h in range(8):\n",
    "        for w in range(8):\n",
    "            targetclass = target4[n, h, w]\n",
    "            target = m(conv(data))[n, targetclass, h, w]\n",
    "            nll += target\n",
    "print(-nll/(5*8*8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71586759-40cb-4fc7-a0fe-95edd34665e6",
   "metadata": {},
   "source": [
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2a50e-9c91-4f5b-9c1b-dbad045634ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas:\n",
    "# - visualization of 100 gradient descent operations on the weight matrix\n",
    "# - reprogram Conv2d layer for an example in tinygrad terminology using Value class\n",
    "# - python script to github (own makemore babylm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53b35990-24f3-44f7-9575-b954f9e887bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "## visualization of 100 gradient descent operations on the weight matrix\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator = g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fc850fd2-89e4-442b-9716-dc79e7840452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio.v3 as iio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "079bdc4d-ba4f-41a5-93d4-a33cef1d1bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4728763103485107\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 100\n",
    "for k in range(n):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims = True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()#  + 0.01*(W**2).mean()\n",
    "    \n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += - 50* W.grad\n",
    "    \n",
    "    plt.imshow(W.data.numpy())\n",
    "    plt.savefig(f\"{k}.jpg\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1683267c-fc63-476f-9ce9-6b5c0c503309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames = np.stack([iio.imread(f\"{k}.jpg\") for k in range(n)], axis=0)\n",
    "iio.imwrite(\"weightmatrix.gif\", frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca5b70-e288-4934-9373-d92ab3966994",
   "metadata": {},
   "source": [
    "<img src=\"weightmatrix.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bfce8d94-e4e3-4e6a-8ca6-4cfba8d35dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reprogram Conv2d layer for an example in tinygrad terminology using Value class\n",
    "# I do it from scratch\n",
    "input = torch.randn(5, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, 4, (3, 3))\n",
    "output = conv(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "180efd31-bbcf-45e1-9634-41a2acf0bc63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape:  torch.Size([5, 16, 10, 10])\n",
      "output.shape:  torch.Size([5, 4, 8, 8])\n",
      "conv.bias.shape:  torch.Size([4])\n",
      "conv.weight.shape:  torch.Size([4, 16, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print('input.shape: ', input.shape)\n",
    "print('output.shape: ', output.shape)\n",
    "print('conv.bias.shape: ', conv.bias.shape)\n",
    "print('conv.weight.shape: ', conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fdf6c9b6-aa7e-41b5-9190-2f2d75ff84f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0089, -0.4342,  0.0485, -0.0622, -0.2543,  0.1707,  0.0696,  0.3200],\n",
       "        [-0.9803,  0.3918,  1.3116, -1.3458, -1.2828, -0.1525, -0.1725,  0.3693],\n",
       "        [ 0.3952,  0.4548, -0.2557, -0.3474, -0.1778,  0.0699, -0.3526,  0.4009],\n",
       "        [-0.3194, -0.1142, -0.9993,  0.4916,  0.1896, -0.4053, -0.9748, -0.6431],\n",
       "        [-0.0151,  0.1367,  0.3428, -0.0445,  0.7619, -0.5679, -0.0166,  0.1496],\n",
       "        [ 0.0639,  0.4096,  0.0959,  0.7384, -0.7021, -0.1635, -0.6977,  0.1653],\n",
       "        [ 0.0325,  1.0574,  0.6667, -0.3198, -0.7068, -0.2384, -0.2920, -0.2000],\n",
       "        [-0.9656,  0.2653, -0.0128,  0.7089, -0.6193, -0.0621,  0.3900, -0.2225]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,0,:,:] # N = 1, C_out = 1, one entry of an 8 by 8 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f40a60d0-b1f0-45cb-a2b6-4e36bdc769ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hardcoded dimensions tensor1 3,3 and tensor2 10,10\n",
    "# note that tensor[1,1,:,:] has shape of length 2, not 4 (dimensions are dropped by default)\n",
    "# output has hardcoded dimension 8,8\n",
    "def crosscorr(tensor1, tensor2):\n",
    "    outtensor = torch.zeros(8,8)\n",
    "    for irow in range(8):\n",
    "        for icol in range(8):\n",
    "            tensor1expanded = torch.zeros(10,10)\n",
    "            tensor1expanded[irow:(irow+3), icol:(icol+3)] = tensor1\n",
    "            outvalue = (tensor1expanded * tensor2).sum()\n",
    "            outtensor[irow, icol] = outvalue\n",
    "    return outtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6e845cd2-591a-410a-8dd6-93a8e5ac6fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0089, -0.4342,  0.0485, -0.0622, -0.2543,  0.1707,  0.0696,  0.3200],\n",
      "        [-0.9803,  0.3918,  1.3116, -1.3458, -1.2828, -0.1525, -0.1725,  0.3693],\n",
      "        [ 0.3952,  0.4548, -0.2557, -0.3474, -0.1778,  0.0699, -0.3526,  0.4009],\n",
      "        [-0.3194, -0.1142, -0.9993,  0.4916,  0.1896, -0.4053, -0.9748, -0.6431],\n",
      "        [-0.0151,  0.1367,  0.3428, -0.0445,  0.7619, -0.5679, -0.0166,  0.1496],\n",
      "        [ 0.0639,  0.4096,  0.0959,  0.7384, -0.7021, -0.1635, -0.6977,  0.1653],\n",
      "        [ 0.0325,  1.0574,  0.6667, -0.3198, -0.7068, -0.2384, -0.2920, -0.2000],\n",
      "        [-0.9656,  0.2653, -0.0128,  0.7089, -0.6193, -0.0621,  0.3900, -0.2225]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This calculation reproduces the 2d convolution for output channel c_out=1 of 4 and sample n=1 of 5\n",
    "out = torch.zeros(8,8)\n",
    "out += conv.bias[0] # start with bias, same for all entries in the 8 by 8 matrix\n",
    "c_out = 0\n",
    "n = 0\n",
    "for k in range(16):\n",
    "    out += crosscorr(conv.weight[n, k, :, :], input[n, k, :, :])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393b807-714f-4ffc-8371-216b1281c28c",
   "metadata": {},
   "source": [
    "Note that the first sum in https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html for the definition of the 2d convolution is meant to be tensors. Everything has dimension 8 (in this example this is the number of output 'channels').\n",
    "\n",
    "The link to Wikipedia for the definition of cross-correlation was not helpful to me. I guessed that it is this sliding sum over the product of (kernel-)weights times the data (thus has dimension 8 by 8, because you can fit 8 non-overlapping blocks of size 3 in something of length 10).\n",
    "\n",
    "The very end of the documentation for 2dConv shows how to access the weights and biasses (conv.weight and conv.bias).\n",
    "\n",
    "The final convolution is the average (or rather: sum) of the individual cross-correlations.\n",
    "\n",
    "Note that the gradient information does not match (slice vs add backward). What does this mean?\n",
    "\n",
    "Open question for me: What is the interpretation of 'channels'? Is it 'number of grayscale colors' for input channels) and 'number of classes' for output channels? Then each entry in the convolution is the average of all colors and all points in the immediate vincinity of a given pixel?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
